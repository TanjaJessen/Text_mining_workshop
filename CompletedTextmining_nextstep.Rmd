---
title: "Textmining Next Step"
author: "Max Odsbjerg Pedersen and Tanja Jessen"
date: "26/10 - 2021"
output: html_document
---

In the previous notebook we loaded the data from St. Croix Avis 1878, which is a data frame of 16 columns. 
Then we added a column with the month each article was released and finally we made the data tidy by taking the text and breaking it into individual words with one word pr row. We start out by doing all this again before we can move on.

At first we need to load in the packages:
```{r}
library(tidyverse)
library(tidytext)
library(lubridate)
library(ggwordcloud)
```

Then we can prepare the data.

Load the data:
```{r}
croix <- read_csv("http://labs.statsbiblioteket.dk/labsapi/api/aviser/export/fields?query=py%3A1878%20AND%20familyId%3Astcroixavisdvi&fields=link&fields=recordID&fields=timestamp&fields=pwa&fields=cer&fields=fulltext_org&fields=pageUUID&fields=editionUUID&fields=titleUUID&fields=editionId&fields=familyId&fields=newspaper_page&fields=newspaper_edition&fields=lplace&fields=location_name&fields=location_coordinates&max=5000&structure=header&structure=content&format=CSV")
```

Add a column with the months:
```{r}
croix %>% 
  mutate(m = month(timestamp)) -> croix
```

Tidy the data:
```{r}
croix %>% 
  unnest_tokens(word, fulltext_org) -> croix_tidy
```

Count the words of each month:
```{r}
croix_tidy %>% 
  count(m, word, sort = TRUE)
```

## Term Frequency - inversed document frequency

In the previous notebook we removed the stop words, the most commonly used words, and then examined the most frequent words. In this notebook we are going to take another approach. 
The method we will be employing is the term frequency - inversed document frequency. This method can be used to create little "summaries" of documents within a corpus by extracting the words that are most significant to each document. By doing this we can create a so-called distant reading of a large data corpus. In our case the corpus is the newspaper data from St. Croix Avis from the year 1878. Even though the newspaper was only released every Wednesday and Saturday the data is still so large that it would be cumbersome to read it all with our human eyes (close reading). So with the St. Croix Avis newspapers from 1878 as our data corpus what are our documents then? In this example we will use the months within the year 1878 as documents. Using the term frequency - inversed document frequency (tf-idf) we will create small "summaries" of the significant words within each months.

The first step is finding a measurement that will allow us to compare the frequency of words across the months. We can do this by calculating the word’s, or the term’s, frequency: 

$$\text{Frequence}=\frac{n_{term}}{N_{month}} $$
Before we can take this step, we need R to count how many words there are in each month. This is done by using the function `group_by` followed by `summarise`:
```{r}
croix_tidy %>% 
  count(m, word, sort = TRUE) %>% 
  group_by(m) %>% 
  summarise(total = sum(n)) -> total_words

total_words
```


Then we add the total number of words to our data frame, which we do with `left_join`:

```{r}
croix_tidy %>%
  count(m, word, sort = TRUE) %>% 
  left_join(total_words, by = "m") -> croix_counts
```


```{r}
croix_counts
```
Now we have the number we need to calculate the frequency of the words. Below we are calculating the word “the” in August(8).



$${\text{Frekvens for "the" in 8}}=\frac{3934}{64822}=0.06068927$$
By calculating the frequency of the terms, we can compare them across each month. However, it is not terribly interesting comparing the word “the” between months. Therefore, we need a way to “punish” words that occur frequently in all months. To achieve this, we are using inversed document frequency(idf):

$$\textrm{idf}(term)=\ln(\frac{n_{\text{document}}}{n_{\text{documents containing term}}})$$

$$\textrm{idf}(the)=\ln(\frac{12}{12})=0$$
Thus we punish words that occur with great frequency in all months or many months. Words that occur in all months cannot really tell us much about a particular month. Those words will have an idf of 0 resulting in a tf_idf value that is also 0, because this is defined by multiplying tf with idf.


Luckily, R can easily calculate tf and tf_idf for all the words by using the bind_tf_idf function:

```{r}
croix_counts %>% 
  bind_tf_idf(word, m, n) -> croix_tfidf

croix_tfidf
```
Nonetheless we still do not see any interesting words. This is because R lists all the words in an ascending order – lowest to highest. Instead, we will ask it to list them in a descending order – highest to lowest tf_idf:

```{r}
croix_tfidf %>% 
  arrange(desc(tf_idf))
```
Many people who have dipped their toes in the text mining/data mining sea will have seen wordclouds showing the most used words in a text. In this visualization we are going to create a wordcloud for each month showing the words with the highest tf_idf from that month. By doing so we will get a nice overview of what words are specific and important to each month. Remember that words which appear alot across the months will not show here. 
```{r}
croix_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(m) %>% 
  top_n(8) %>% 
  ungroup %>%
  ggplot(aes(label = word, size = tf_idf, color = tf_idf)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 5) +
  theme_minimal() +
  facet_wrap(~m, ncol = 4, scales = "free") +
  scale_color_gradient(low = "darkgoldenrod2", high = "darkgoldenrod4") +
  labs(
      title = "St. Croix Avis: most important words pr. month",
       subtitle = "Importance determined by term frequency (tf) - inversed document frequency(idf)",
      caption = "Data from Mediestream Experimental API")
```
Let's see what is going on with "rioters" in October. Be using our intial dataframe we can specify that we are only interested in October and thereafter that we only want to see the articles that has the word "rioters" in them.

```{r}
croix %>% 
  filter(m == 10) %>% 
  filter(str_detect(fulltext_org, "rioters")) %>% 
  select(fulltext_org, timestamp, link)
```

Congratulations! You have completed your very first text mining task and created an output! You are now ready to venture further into the world of tidy text mining. This short introduction was based on the [Tidy Text Mining with R](https://www.tidytextmining.com)-book(Chapter 1 and 3). Now that you know how to use an R-markdown you can use the book to explore their methods! 

